# Converting Informal Persian Text to Formal Persian  

## Overview  
This project explores the task of **converting informal Persian text into formal Persian** using deep learning models.  I approached this task as a **sequence-to-sequence (seq2seq) problem**, similar to **machine translation** â€” where the "source language" is informal Persian and the "target language" is formal Persian.

With this approach in mind, I implemented and compared **two widely used seq2seq architectures**:
- **Seq2Seq LSTMs** (a classical recurrent approach)  
- **Transformers** (state of the art attention-based models)  

Both were tested with three tokenization approaches: **character-level**, **word-level**, and **subword-level**.  

Combining these two models with the mentioned tokenization approaches created a six-part comparative study:  

- **Char + LSTM**  
- **Char + Transformer**  
- **Word + LSTM**  
- **Word + Transformer**  
- **Subword + LSTM**  
- **Subword + Transformer**  


## Motivation  
A large portion of Persian text available online is written informally, making it difficult to use directly for NLP tasks. By transforming informal Persian into its formal form, this project provides a **preprocessing step** which can be used for building better Persian NLP tools and training large language models on cleaner and more standardized data.


## Dataset  
I used the dataset presented in **Developing an Informal-Formal Persian Corpus**. You can find full details in the paper: https://aclanthology.org/2025.abjadnlp-1.6/  

- **Size:** ~50,000 informal-formal sentence pairs  
- **Sources:** Social media, messaging apps, websites, blogs, books, movies  
- **Characteristics:**  
  - Lexical and syntactic variation (50% require structural changes)  
  - Wide coverage of colloquial Persian  
  - Variable sentence lengths   

**Example:**  

| Informal | Formal |  
|----------|--------|  
| Ù…Ù† Ø¯ÙˆØ³ Ø¯Ø§Ø±Ù… Ø¨Ø±Ù… Ø®ÙˆÙ†Ù‡ Ø¯Ø±Ø³ Ø¨Ø®ÙˆÙ†Ù…. | Ù…Ù† Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù… Ú©Ù‡ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø±ÙˆÙ… ØªØ§ Ø¯Ø±Ø³ Ø¨Ø®ÙˆØ§Ù†Ù…. |  
| Ù…ÛŒØªÙˆÙ†ÛŒ Ù…Ù†Ùˆ Ø¨Ø¨Ø±ÛŒ Ø®ÙˆÙ†Ù…ÙˆÙ† ÛŒÚ©Ù… Ù†ÙˆÙ† ÙˆØ±Ø¯Ø§Ø±Ù…ØŸ | Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ù…Ù† Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ù†Ù‡â€ŒÙ…Ø§Ù† Ø¨Ø¨Ø±ÛŒ ØªØ§ Ú©Ù…ÛŒ Ù†Ø§Ù† Ø¨Ø±Ø¯Ø§Ø±Ù…ØŸ |


## Models & Tokenization  

### ğŸ”¹ Why LSTMs and Transformers?  
Since the task is framed as **sequence-to-sequence transformation**, the natural model choices were:  

- **LSTM-based Seq2Seq models** â†’ classical architecture for machine translation, capable of handling variable-length input and output sequences.  
- **Transformer models** â†’ more recent architecture relying on attention mechanisms, better at capturing long-range dependencies and parallelizing training.  

This allowed me to compare a **baseline seq2seq approach (LSTMs)** against the **modern state-of-the-art (Transformers)** in the context of Persian text transformation.  

### ğŸ”¹ Seq2Seq LSTM  
- Embeddings, multi-layer LSTM encoder/decoder, dropout, fully connected output.  

### ğŸ”¹ Transformer  
- Implemented using `nn.Transformer` (PyTorch).  
- Positional encodings, padding & look-ahead masks.  

### ğŸ”¹ Tokenization Methods  
- **Character-level** â†’ small vocab, long sequences, weak semantics.  
- **Word-level** â†’ strong semantics, large vocab, OOV problems.  
- **Subword-level** (SentencePiece Unigram) â†’ balance of vocabulary size + semantic coverage.  


## Results  

### Metrics  
- **CER (Character Error Rate):** measures the percentage of characters that are wrong compared to the reference.  
- **WER (Word Error Rate):** measures the percentage of words that are wrong compared to the reference.  
Lower is better for both.  

### Summary (Test Set Averages)  

| Tokenization | Model | CER (%) | WER (%) |  
|--------------|-------|-----|-----|  
| Char | LSTM | 98.75% | 126.03% = 100% |  
| **Char** | **Transformer** | **15.44%** | **26.11%** |  
| Subword | LSTM | 59.41% | 79.64% |  
| **Subword** | **Transformer** | **32.58%** | **43.24%** |  
| Word | LSTM | 72.85% | 87.91% |  
| Word | Transformer | 46.59% | 58.10% |  

> These are the **best test-set results** obtained among all model checkpoints (for each tokenization + model type).  
> In each tokenization folder/notebook, you can find additional models saved by best validation loss and best validation CER, covering evaluation results for training, validation, and test sets.  

âœ… **Best performing models:**  
- **Char-level Transformer** (lowest CER/WER overall)  
- **Subword-level Transformer** (best balance between vocabulary size & semantic handling)  


## Example Outputs  

| Input (Informal) | Target (Formal) | Prediction (Transformer-Subword best test-set results) |  CER | WER |
|------------------|-----------------|---------------------------------|-----|-----| 
| Ø¹Ù…Ø¯Ø§ Ù…ÛŒØ®ÙˆØ§Ø¯ Ø­Ø±ØµÙ…ÙˆÙ† Ø¨Ø¯Ù‡ | Ø¹Ù…Ø¯Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯ Ù…Ø§ Ø±Ø§ Ø­Ø±Øµ Ø¨Ø¯Ù‡Ø¯ | Ø¹Ù…Ø¯Ø§ Ù…ÛŒ Ø®ÙˆØ§Ù‡Ø¯ Ù…Ø§ Ø±Ø§ Ø­Ø±Øµ Ø¨Ø¯Ù‡Ø¯ | 0% | 0% |
Ú©Ù„Ø§Ù‡ØªÙˆ Ø³Ù Ø¨Ú†Ø³Ø¨ Ù‡Ù…Ø³Ø§ÛŒØªÙˆ Ø¯Ø²Ø¯ Ù†Ú©Ù† | Ú©Ù„Ø§Ù‡Øª Ø±Ø§ Ø³ÙØª Ø¨Ú†Ø³Ø¨ Ùˆ Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒØ§Øª Ø±Ø§ Ø¯Ø²Ø¯ Ù†Ú©Ù† | Ú©Ù„Ø§Ù‡Øª Ø±Ø§ Ø²Ù†Ø¨ÙˆØ± Ø¹Ø³Ù„ Ù‡Ù…Ø³Ø§ÛŒÙ‡ Ø§Øª Ø±Ø§ Ø¯Ø²Ø¯ Ù†Ú©Ù† | 25% | 30% |

## Reflections  

This project gave me hands-on experience in:  

- Building **NLP pipelines from scratch** (data preprocessing, tokenization, batching, padding).  
- Implementing **seq2seq models** with LSTMs and Transformers in PyTorch.  
- Handling **evaluation metrics (CER, WER)** and saving/loading best checkpoints.  
- Debugging model behaviors (e.g., handling `<BOS>`/`<EOS>` tokens, masking).  

**Key insights:**  
- Transformers were significantly better than LSTMs for this task.  
- Tokenization strongly affects results â€” **subword tokenization gives the best balance**.  
- Character-level Transformers surprisingly achieved the best raw performance, likely due to handling spelling variants in informal Persian.  

**Future work:**  
- Implement **beam search decoding** instead of greedy decoding.  
- Use **pretrained embeddings** or pretrained language models (e.g., multilingual BERT).  

## Running the Code  

    All parts are Jupyter Notebooks (Ran on Google Colab).  
    Place the dataset Excel file either:  
      - directly in Colab via upload, or  
      - in the same directory as the notebook (If run locally).  

**Dependencies:**  
- Python  
- PyTorch  
- Jiwer  
- SentencePiece  
- NumPy, Pandas  


## Repository Structure  

    char/         â†’ lstm_char.ipynb, transformer_char.ipynb
    subword/      â†’ lstm_subword.ipynb, transformer_subword.ipynb
    word/         â†’ lstm_word.ipynb, transformer_word.ipynb
    exportStatements.xlsx â†’ dataset 