{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from jiwer import wer, cer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = './output_2_trasnformer/transformer'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for saving/loading\n",
    "DATASET_PATH = './exportStatements.xlsx'\n",
    "TOKENIZER_MODEL_PATH = os.path.join(OUTPUT_DIR, 'sentencepiece.model')\n",
    "TOKENIZER_VOCAB_PATH = os.path.join(OUTPUT_DIR, 'sentencepiece.vocab')\n",
    "PREPROCESSED_DATA_PATH = os.path.join(OUTPUT_DIR, 'preprocessed_data.pkl')\n",
    "BEST_MODEL_PATH = os.path.join(OUTPUT_DIR, 'best_transformer_model.pt')\n",
    "LOSS_PLOT_PATH = os.path.join(OUTPUT_DIR, 'transformer_loss_plot.png')\n",
    "WER_PLOT_PATH = os.path.join(OUTPUT_DIR, 'wer_plot.png')\n",
    "CER_PLOT_PATH = os.path.join(OUTPUT_DIR, 'cer_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_excel(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'inFormalForm' and 'FormalForm'\n",
    "print(\"Missing values in 'inFormalForm':\", df['inFormalForm'].isnull().sum())\n",
    "print(\"Missing values in 'FormalForm':\", df['FormalForm'].isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in 'inFormalForm' and 'FormalForm'\n",
    "initial_length = len(df)\n",
    "df = df.dropna(subset=['inFormalForm', 'FormalForm']).reset_index(drop=True)\n",
    "final_length = len(df)\n",
    "\n",
    "df['inFormalForm'] = df['inFormalForm'].astype(str)\n",
    "df['FormalForm'] = df['FormalForm'].astype(str)\n",
    "\n",
    "print(f\"Dropped {initial_length - final_length} rows due to missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets (80%, 10%, 10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if tokenizer model exists\n",
    "if not os.path.exists(TOKENIZER_MODEL_PATH):\n",
    "    print('Training SentencePiece tokenizer...')\n",
    "    # Save combined sentences for tokenizer training\n",
    "    all_sentences_path = os.path.join(OUTPUT_DIR, 'all_sentences.txt')\n",
    "    with open(all_sentences_path, 'w', encoding='utf-8') as f:\n",
    "        for sent in pd.concat([train_df['inFormalForm'], train_df['FormalForm']]):\n",
    "            f.write(sent.strip() + '\\n')\n",
    "\n",
    "    # Adjusted vocab_size to 27000\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=all_sentences_path,\n",
    "        model_prefix=os.path.join(OUTPUT_DIR, 'sentencepiece'),\n",
    "        vocab_size=27000,\n",
    "        model_type='unigram',\n",
    "        character_coverage=1.0,\n",
    "        pad_id=0,      # ID for <pad>\n",
    "        unk_id=1,      # ID for <unk> (reserved, do not redefine)\n",
    "        bos_id=2,      # ID for <s> (reserved)\n",
    "        eos_id=3,      # ID for </s> (reserved)\n",
    "        user_defined_symbols=['<pad>']  # Only include <pad> as user-defined symbol\n",
    "    )\n",
    "    print('Tokenizer trained and saved.')\n",
    "else:\n",
    "    print('Loading existing tokenizer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(TOKENIZER_MODEL_PATH)\n",
    "print(f'Vocabulary size: {sp.get_piece_size()}')\n",
    "\n",
    "# Special token IDs\n",
    "PAD_IDX = sp.piece_to_id('<pad>')   # Should be 0\n",
    "UNK_IDX = sp.piece_to_id('<unk>')   # Should be 1\n",
    "BOS_IDX = sp.piece_to_id('<s>')     # Should be 2\n",
    "EOS_IDX = sp.piece_to_id('</s>')    # Should be 3\n",
    "\n",
    "## Special token IDs\n",
    "# PAD_IDX = sp.pad_id()\n",
    "# UNK_IDX = sp.unk_id()\n",
    "# BOS_IDX = sp.bos_id()\n",
    "# EOS_IDX = sp.eos_id()\n",
    "PAD_IDX, UNK_IDX, BOS_IDX, EOS_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length (based on dataset)\n",
    "def get_max_len(df_list):\n",
    "    max_len = 0\n",
    "    for df in df_list:\n",
    "        lengths = df['inFormalForm'].apply(lambda x: len(sp.EncodeAsIds(x)) + 2)  # +2 for BOS and EOS\n",
    "        lengths_trg = df['FormalForm'].apply(lambda x: len(sp.EncodeAsIds(x)) + 2)\n",
    "        max_len = max(max_len, lengths.max(), lengths_trg.max())\n",
    "    return max_len\n",
    "\n",
    "MAX_LEN = get_max_len([train_df, val_df, test_df])\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if preprocessed data exists\n",
    "if not os.path.exists(PREPROCESSED_DATA_PATH):\n",
    "    print('Preprocessing data...')\n",
    "    # Preprocess and tokenize all sentences\n",
    "    def preprocess_data(df, sp, max_len=MAX_LEN):\n",
    "        src_texts = df['inFormalForm'].tolist()\n",
    "        trg_texts = df['FormalForm'].tolist()\n",
    "        src_sequences = []\n",
    "        trg_sequences = []\n",
    "        for src, trg in zip(src_texts, trg_texts):\n",
    "            src_ids = [BOS_IDX] + sp.EncodeAsIds(src) + [EOS_IDX]\n",
    "            trg_ids = [BOS_IDX] + sp.EncodeAsIds(trg) + [EOS_IDX]\n",
    "            # Pad or truncate sequences\n",
    "            src_ids = src_ids[:max_len] + [PAD_IDX] * max(0, max_len - len(src_ids))\n",
    "            trg_ids = trg_ids[:max_len] + [PAD_IDX] * max(0, max_len - len(trg_ids))\n",
    "            src_sequences.append(src_ids)\n",
    "            trg_sequences.append(trg_ids)\n",
    "        return src_sequences, trg_sequences\n",
    "    \n",
    "    # Tokenize and preprocess data\n",
    "    train_src, train_trg = preprocess_data(train_df, sp)\n",
    "    val_src, val_trg = preprocess_data(val_df, sp)\n",
    "    test_src, test_trg = preprocess_data(test_df, sp)\n",
    "\n",
    "    # Save preprocessed data\n",
    "    with open(PREPROCESSED_DATA_PATH, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'train_src': train_src,\n",
    "            'train_trg': train_trg,\n",
    "            'val_src': val_src,\n",
    "            'val_trg': val_trg,\n",
    "            'test_src': test_src,\n",
    "            'test_trg': test_trg,\n",
    "            'MAX_LEN': MAX_LEN\n",
    "        }, f)\n",
    "    print('Preprocessed data saved.')\n",
    "else:\n",
    "    print('Loading preprocessed data...')\n",
    "    # Load preprocessed data\n",
    "    with open(PREPROCESSED_DATA_PATH, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        train_src = data['train_src']\n",
    "        train_trg = data['train_trg']\n",
    "        val_src = data['val_src']\n",
    "        val_trg = data['val_trg']\n",
    "        test_src = data['test_src']\n",
    "        test_trg = data['test_trg']\n",
    "        MAX_LEN = data['MAX_LEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sequences, trg_sequences):\n",
    "        self.src_sequences = src_sequences\n",
    "        self.trg_sequences = trg_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = torch.tensor(self.src_sequences[idx], dtype=torch.long)\n",
    "        trg_ids = torch.tensor(self.trg_sequences[idx], dtype=torch.long)\n",
    "        return src_ids, trg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function to create masks and pad sequences\n",
    "def collate_fn(batch, pad_idx):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=pad_idx, batch_first=True)\n",
    "    trg_batch = torch.nn.utils.rnn.pad_sequence(trg_batch, padding_value=pad_idx, batch_first=True)\n",
    "    return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_trg)\n",
    "val_dataset = TranslationDataset(val_src, val_trg)\n",
    "test_dataset = TranslationDataset(test_src, test_trg)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, PAD_IDX))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=lambda x: collate_fn(x, PAD_IDX))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=lambda x: collate_fn(x, PAD_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate subsequent masks for target\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones((sz, sz), device=DEVICE) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create padding masks\n",
    "def create_mask(src, tgt, pad_idx):\n",
    "    src_seq_len = src.size(1)\n",
    "    tgt_seq_len = tgt.size(1)\n",
    "\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "\n",
    "    src_padding_mask = (src == pad_idx)\n",
    "    tgt_padding_mask = (tgt == pad_idx)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model Definition\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size,\n",
    "                 nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=512,\n",
    "                 dropout=0.1, max_len=MAX_LEN, pad_idx=PAD_IDX):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Token embedding layers\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, emb_size, padding_idx=pad_idx)\n",
    "\n",
    "        # Learned positional embeddings\n",
    "        self.src_pos_embedding = nn.Embedding(max_len, emb_size)\n",
    "        self.tgt_pos_embedding = nn.Embedding(max_len, emb_size)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        # src and tgt shape: [batch_size, seq_len]\n",
    "        src_seq_len = src.size(1)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        src_positions = torch.arange(0, src_seq_len, device=src.device).unsqueeze(0).expand(src.size(0), -1)\n",
    "        tgt_positions = torch.arange(0, tgt_seq_len, device=tgt.device).unsqueeze(0).expand(tgt.size(0), -1)\n",
    "\n",
    "        # Embed and encode source\n",
    "        src_emb = self.src_embedding(src) + self.src_pos_embedding(src_positions)\n",
    "        src_emb = src_emb * math.sqrt(self.emb_size)\n",
    "        # Embed and encode target\n",
    "        tgt_emb = self.tgt_embedding(tgt) + self.tgt_pos_embedding(tgt_positions)\n",
    "        tgt_emb = tgt_emb * math.sqrt(self.emb_size)\n",
    "\n",
    "        # Transformer\n",
    "        output = self.transformer(src_emb.transpose(0,1), tgt_emb.transpose(0,1),\n",
    "                                  src_mask, tgt_mask, None,\n",
    "                                  src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        output = output.transpose(0,1)\n",
    "        logits = self.generator(output)\n",
    "        return logits\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src_seq_len = src.size(1)\n",
    "        src_positions = torch.arange(0, src_seq_len, device=src.device).unsqueeze(0).expand(src.size(0), -1)\n",
    "        src_emb = self.src_embedding(src) + self.src_pos_embedding(src_positions)\n",
    "        src_emb = src_emb * math.sqrt(self.emb_size)\n",
    "        return self.transformer.encoder(src_emb.transpose(0,1), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_positions = torch.arange(0, tgt_seq_len, device=tgt.device).unsqueeze(0).expand(tgt.size(0), -1)\n",
    "        tgt_emb = self.tgt_embedding(tgt) + self.tgt_pos_embedding(tgt_positions)\n",
    "        tgt_emb = tgt_emb * math.sqrt(self.emb_size)\n",
    "        return self.transformer.decoder(tgt_emb.transpose(0,1), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "VOCAB_SIZE = sp.get_piece_size()\n",
    "SRC_VOCAB_SIZE = VOCAB_SIZE\n",
    "TGT_VOCAB_SIZE = VOCAB_SIZE\n",
    "\n",
    "# Hyperparameters (as per your instructions)\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "emb_size = 256\n",
    "nhead = 8\n",
    "dim_feedforward = 256\n",
    "dropout = 0.1  # Adjust dropout rate as needed\n",
    "\n",
    "\n",
    "model = Seq2SeqTransformer(num_encoder_layers, num_decoder_layers, emb_size,\n",
    "                           nhead, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    "                           dim_feedforward, dropout, MAX_LEN, PAD_IDX).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# # Initialize optimizer and loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Adjusted learning rate for Transformer\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_wer(model, dataloader, sp, max_batches=None):\n",
    "    model.eval()\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    batches_processed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, trg[:, :-1], PAD_IDX)\n",
    "\n",
    "            output = model(src, trg[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "            output = output.argmax(dim=-1)\n",
    "\n",
    "            trg_out = trg[:, 1:]  # Remove first token (<s>) for target\n",
    "            output = output.cpu().tolist()\n",
    "            trg_out = trg_out.cpu().tolist()\n",
    "\n",
    "            for pred_ids, trg_ids in zip(output, trg_out):\n",
    "                # Remove PAD and special tokens\n",
    "                pred_ids = [idx for idx in pred_ids if idx not in [PAD_IDX, EOS_IDX, UNK_IDX]]\n",
    "                trg_ids = [idx for idx in trg_ids if idx not in [PAD_IDX, EOS_IDX, UNK_IDX]]\n",
    "\n",
    "                pred_sentence = sp.DecodeIds(pred_ids)\n",
    "                trg_sentence = sp.DecodeIds(trg_ids)\n",
    "\n",
    "                cer_score = cer(trg_sentence, pred_sentence)\n",
    "                wer_score = wer(trg_sentence, pred_sentence)\n",
    "\n",
    "                cer_scores.append(cer_score)\n",
    "                wer_scores.append(wer_score)\n",
    "\n",
    "            batches_processed += 1\n",
    "            if max_batches and batches_processed >= max_batches:\n",
    "                break\n",
    "\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    return avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "train_wers = []\n",
    "valid_wers = []\n",
    "train_cers = []\n",
    "valid_cers = []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for src, trg in tqdm(train_loader, desc=f'Training Epoch {epoch}/{N_EPOCHS}'):\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "\n",
    "        # Create masks\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, trg[:, :-1], PAD_IDX)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        # output: [batch_size, tgt_len - 1, vocab_size]\n",
    "        # trg_out: [batch_size, tgt_len - 1]\n",
    "        trg_out = trg[:, 1:]\n",
    "\n",
    "        output = output.reshape(-1, TGT_VOCAB_SIZE)\n",
    "        trg_out = trg_out.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg_out)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(val_loader, desc=f'Validation Epoch {epoch}/{N_EPOCHS}'):\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, trg[:, :-1], PAD_IDX)\n",
    "\n",
    "            output = model(src, trg[:, :-1], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "            trg_out = trg[:, 1:]\n",
    "\n",
    "            output = output.reshape(-1, TGT_VOCAB_SIZE)\n",
    "            trg_out = trg_out.reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg_out)\n",
    "            epoch_valid_loss += loss.item()\n",
    "\n",
    "    epoch_valid_loss /= len(val_loader)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "\n",
    "    # Evaluate WER\n",
    "    valid_cer, valid_wer = evaluate_wer(model, val_loader, sp, max_batches=5)\n",
    "    valid_wers.append(valid_wer)\n",
    "    valid_cers.append(valid_cer)\n",
    "    \n",
    "\n",
    "    train_subset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, PAD_IDX))\n",
    "    train_cer, train_wer = evaluate_wer(model, train_subset_loader, sp, max_batches=5)\n",
    "    train_wers.append(train_wer)\n",
    "    train_cers.append(train_cer)\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f'Validation loss improved. Model saved to {BEST_MODEL_PATH}.')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(int(end_time - start_time), 60)\n",
    "\n",
    "    print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {epoch_train_loss:.3f}')\n",
    "    print(f'\\tValid Loss: {epoch_valid_loss:.3f}')\n",
    "    print(f'\\tTrain WER: {train_wer:.4f}')\n",
    "    print(f'\\tValid WER: {valid_wer:.4f}')\n",
    "    print(f'\\tTrain CER: {train_cer:.4f}')\n",
    "    print(f'\\tValid CER: {valid_cer:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(valid_losses) + 1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(LOSS_PLOT_PATH)\n",
    "plt.show()\n",
    "print(f'Loss plot saved to {LOSS_PLOT_PATH}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WER over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_wers) + 1), train_wers, label='Train WER')\n",
    "plt.plot(range(1, len(valid_wers) + 1), valid_wers, label='Validation WER')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('WER')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation WER Over Epochs')\n",
    "plt.savefig(WER_PLOT_PATH)\n",
    "plt.show()\n",
    "print(f'WER plot saved to {WER_PLOT_PATH}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WER over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_cers) + 1), train_wers, label='Train CER')\n",
    "plt.plot(range(1, len(valid_cers) + 1), valid_wers, label='Validation CER')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CER')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation CER Over Epochs')\n",
    "plt.savefig(CER_PLOT_PATH)\n",
    "plt.show()\n",
    "print(f'CER plot saved to {CER_PLOT_PATH}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inference\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        tgt_mask = generate_square_subsequent_mask(ys.size(1)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0,1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        next_word = torch.argmax(prob, dim=1).item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, sp, device, max_len=MAX_LEN, decoding_strategy='greedy'):\n",
    "    model.eval()\n",
    "    tokens = [BOS_IDX] + sp.EncodeAsIds(sentence) + [EOS_IDX]\n",
    "    tokens = tokens[:max_len]\n",
    "    src = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    src_mask = (torch.zeros(src.shape[1], src.shape[1])).type(torch.bool).to(device)\n",
    "    if decoding_strategy == 'greedy':\n",
    "        tgt_tokens = greedy_decode(model, src, src_mask, max_len, BOS_IDX).flatten()\n",
    "    elif decoding_strategy == 'beam':\n",
    "        # Implement beam search decoding as needed\n",
    "        raise NotImplementedError(\"Beam search decoding is not implemented yet.\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid decoding strategy\")\n",
    "    tgt_tokens = tgt_tokens.cpu().numpy()\n",
    "    # Remove BOS token\n",
    "    tgt_tokens = tgt_tokens[1:]\n",
    "    # Stop at EOS token\n",
    "    if EOS_IDX in tgt_tokens:\n",
    "        eos_index = np.where(tgt_tokens == EOS_IDX)[0][0]\n",
    "        tgt_tokens = tgt_tokens[:eos_index]\n",
    "    translation = sp.DecodeIds(tgt_tokens.tolist())\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate CER and WER\n",
    "def calculate_metrics(references, hypotheses):\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        cer_score = cer(ref, hyp)\n",
    "        wer_score = wer(ref, hyp)\n",
    "        cer_scores.append(cer_score)\n",
    "        wer_scores.append(wer_score)\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    return avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate and save results with sorting by CER and WER\n",
    "def evaluate_and_save(model, df, src_sequences, trg_sequences, sp, file_name):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "\n",
    "    for src_ids, trg_ids in tqdm(zip(src_sequences, trg_sequences), total=len(src_sequences), desc=f'Evaluating {file_name}'):\n",
    "        src_sentence = sp.DecodeIds([id for id in src_ids if id not in [BOS_IDX, EOS_IDX, PAD_IDX]])\n",
    "        trg_sentence = sp.DecodeIds([id for id in trg_ids if id not in [BOS_IDX, EOS_IDX, PAD_IDX]])\n",
    "\n",
    "        pred_sentence = translate_sentence(src_sentence, model, sp, DEVICE)\n",
    "        predictions.append(pred_sentence)\n",
    "        cer_score = cer(trg_sentence, pred_sentence)\n",
    "        wer_score = wer(trg_sentence, pred_sentence)\n",
    "        cer_scores.append(cer_score)\n",
    "        wer_scores.append(wer_score)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Source': df['inFormalForm'],\n",
    "        'Target': df['FormalForm'],\n",
    "        'Prediction': predictions,\n",
    "        'CER': cer_scores,\n",
    "        'WER': wer_scores\n",
    "    })\n",
    "    \n",
    "    results_df = results_df.sort_values(by=['CER', 'WER'], ascending=[True, True])\n",
    "\n",
    "    results_path = os.path.join(OUTPUT_DIR, file_name)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    print(f'Results saved to {results_path}')\n",
    "    print(f'Average CER: {avg_cer:.4f}')\n",
    "    print(f'Average WER: {avg_wer:.4f}')\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "print('Best model loaded.')\n",
    "\n",
    "# Evaluate on training data\n",
    "print('Evaluating on training data...')\n",
    "train_results = evaluate_and_save(model, train_df, train_src, train_trg, sp, 'train_results.csv')\n",
    "\n",
    "# Evaluate on validation data\n",
    "print('Evaluating on validation data...')\n",
    "val_results = evaluate_and_save(model, val_df, val_src, val_trg, sp, 'val_results.csv')\n",
    "\n",
    "# Evaluate on test data\n",
    "print('Evaluating on test data...')\n",
    "test_results = evaluate_and_save(model, test_df, test_src, test_trg, sp, 'test_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
