{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from jiwer import wer, cer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = './output_3/lstm'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for saving/loading\n",
    "DATASET_PATH = './exportStatements.xlsx'\n",
    "TOKENIZER_MODEL_PATH = os.path.join(OUTPUT_DIR, 'sentencepiece.model')\n",
    "TOKENIZER_VOCAB_PATH = os.path.join(OUTPUT_DIR, 'sentencepiece.vocab')\n",
    "PREPROCESSED_DATA_PATH = os.path.join(OUTPUT_DIR, 'preprocessed_data.pkl')\n",
    "BEST_MODEL_PATH = os.path.join(OUTPUT_DIR, 'best_lstm_model.pt')\n",
    "LOSS_PLOT_PATH = os.path.join(OUTPUT_DIR, 'lstm_loss_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_excel(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'inFormalForm' and 'FormalForm'\n",
    "print(\"Missing values in 'inFormalForm':\", df['inFormalForm'].isnull().sum())\n",
    "print(\"Missing values in 'FormalForm':\", df['FormalForm'].isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in 'inFormalForm' and 'FormalForm'\n",
    "initial_length = len(df)\n",
    "df = df.dropna(subset=['inFormalForm', 'FormalForm']).reset_index(drop=True)\n",
    "final_length = len(df)\n",
    "\n",
    "df['inFormalForm'] = df['inFormalForm'].astype(str)\n",
    "df['FormalForm'] = df['FormalForm'].astype(str)\n",
    "\n",
    "print(f\"Dropped {initial_length - final_length} rows due to missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets (80%, 10%, 10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if tokenizer model exists\n",
    "if not os.path.exists(TOKENIZER_MODEL_PATH):\n",
    "    print('Training SentencePiece tokenizer...')\n",
    "    # Save combined sentences for tokenizer training\n",
    "    all_sentences_path = os.path.join(OUTPUT_DIR, 'all_sentences.txt')\n",
    "    with open(all_sentences_path, 'w', encoding='utf-8') as f:\n",
    "        for sent in pd.concat([train_df['inFormalForm'], train_df['FormalForm']]):\n",
    "            f.write(sent.strip() + '\\n')\n",
    "\n",
    "    # Adjusted vocab_size to 27000\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=all_sentences_path,\n",
    "        model_prefix=os.path.join(OUTPUT_DIR, 'sentencepiece'),\n",
    "        vocab_size=27000,\n",
    "        model_type='unigram',\n",
    "        character_coverage=1.0,\n",
    "        pad_id=0,      # ID for <pad>\n",
    "        unk_id=1,      # ID for <unk> (reserved, do not redefine)\n",
    "        bos_id=2,      # ID for <s> (reserved)\n",
    "        eos_id=3,      # ID for </s> (reserved)\n",
    "        user_defined_symbols=['<pad>']  # Only include <pad> as user-defined symbol\n",
    "    )\n",
    "    print('Tokenizer trained and saved.')\n",
    "else:\n",
    "    print('Loading existing tokenizer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(TOKENIZER_MODEL_PATH)\n",
    "print(f'Vocabulary size: {sp.get_piece_size()}')\n",
    "\n",
    "# Special token IDs\n",
    "PAD_IDX = sp.piece_to_id('<pad>')   # Should be 0\n",
    "UNK_IDX = sp.piece_to_id('<unk>')   # Should be 1\n",
    "BOS_IDX = sp.piece_to_id('<s>')     # Should be 2\n",
    "EOS_IDX = sp.piece_to_id('</s>')    # Should be 3\n",
    "\n",
    "## Special token IDs\n",
    "# PAD_IDX = sp.pad_id()\n",
    "# UNK_IDX = sp.unk_id()\n",
    "# BOS_IDX = sp.bos_id()\n",
    "# EOS_IDX = sp.eos_id()\n",
    "PAD_IDX, UNK_IDX, BOS_IDX, EOS_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length (based on dataset)\n",
    "def get_max_len(df_list):\n",
    "    max_len = 0\n",
    "    for df in df_list:\n",
    "        lengths = df['inFormalForm'].apply(lambda x: len(sp.EncodeAsIds(x)) + 2)  # +2 for BOS and EOS\n",
    "        lengths_trg = df['FormalForm'].apply(lambda x: len(sp.EncodeAsIds(x)) + 2)\n",
    "        max_len = max(max_len, lengths.max(), lengths_trg.max())\n",
    "    return max_len\n",
    "\n",
    "MAX_LEN = get_max_len([train_df, val_df, test_df])\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if preprocessed data exists\n",
    "if not os.path.exists(PREPROCESSED_DATA_PATH):\n",
    "    print('Preprocessing data...')\n",
    "    # Preprocess and tokenize all sentences\n",
    "    def preprocess_data(df, sp, max_len=MAX_LEN):\n",
    "        src_texts = df['inFormalForm'].tolist()\n",
    "        trg_texts = df['FormalForm'].tolist()\n",
    "        src_sequences = []\n",
    "        trg_sequences = []\n",
    "        for src, trg in zip(src_texts, trg_texts):\n",
    "            src_ids = [BOS_IDX] + sp.EncodeAsIds(src) + [EOS_IDX]\n",
    "            trg_ids = [BOS_IDX] + sp.EncodeAsIds(trg) + [EOS_IDX]\n",
    "            # Pad or truncate sequences\n",
    "            src_ids = src_ids[:max_len] + [PAD_IDX] * max(0, max_len - len(src_ids))\n",
    "            trg_ids = trg_ids[:max_len] + [PAD_IDX] * max(0, max_len - len(trg_ids))\n",
    "            src_sequences.append(src_ids)\n",
    "            trg_sequences.append(trg_ids)\n",
    "        return src_sequences, trg_sequences\n",
    "    \n",
    "    # Tokenize and preprocess data\n",
    "    train_src, train_trg = preprocess_data(train_df, sp)\n",
    "    val_src, val_trg = preprocess_data(val_df, sp)\n",
    "    test_src, test_trg = preprocess_data(test_df, sp)\n",
    "\n",
    "    # Save preprocessed data\n",
    "    with open(PREPROCESSED_DATA_PATH, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'train_src': train_src,\n",
    "            'train_trg': train_trg,\n",
    "            'val_src': val_src,\n",
    "            'val_trg': val_trg,\n",
    "            'test_src': test_src,\n",
    "            'test_trg': test_trg,\n",
    "            'MAX_LEN': MAX_LEN\n",
    "        }, f)\n",
    "    print('Preprocessed data saved.')\n",
    "else:\n",
    "    print('Loading preprocessed data...')\n",
    "    # Load preprocessed data\n",
    "    with open(PREPROCESSED_DATA_PATH, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        train_src = data['train_src']\n",
    "        train_trg = data['train_trg']\n",
    "        val_src = data['val_src']\n",
    "        val_trg = data['val_trg']\n",
    "        test_src = data['test_src']\n",
    "        test_trg = data['test_trg']\n",
    "        MAX_LEN = data['MAX_LEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sequences, trg_sequences):\n",
    "        self.src_sequences = src_sequences\n",
    "        self.trg_sequences = trg_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = torch.tensor(self.src_sequences[idx], dtype=torch.long)\n",
    "        trg_ids = torch.tensor(self.trg_sequences[idx], dtype=torch.long)\n",
    "        return src_ids, trg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_trg)\n",
    "val_dataset = TranslationDataset(val_src, val_trg)\n",
    "test_dataset = TranslationDataset(test_src, test_trg)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM-based Seq2Seq model (merged Encoder and Decoder)\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.encoder = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # Embed source and target sequences\n",
    "        embedded_src = self.dropout(self.embedding(src))\n",
    "        embedded_trg = self.dropout(self.embedding(trg[:, :-1]))  # Remove last token for decoder input\n",
    "\n",
    "        # Encode source sequence\n",
    "        _, (hidden, cell) = self.encoder(embedded_src)\n",
    "\n",
    "        # Decode target sequence\n",
    "        outputs, _ = self.decoder(embedded_trg, (hidden, cell))\n",
    "        predictions = self.fc_out(outputs)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, src, max_len=MAX_LEN):\n",
    "        # Embed source sequence\n",
    "        embedded_src = self.dropout(self.embedding(src))\n",
    "\n",
    "        # Encode source sequence\n",
    "        _, (hidden, cell) = self.encoder(embedded_src)\n",
    "\n",
    "        # Initialize target sequence with <s>\n",
    "        inputs = torch.tensor([BOS_IDX] * src.size(0), dtype=torch.long).unsqueeze(1).to(src.device)\n",
    "        outputs = []\n",
    "        for _ in range(max_len):\n",
    "            embedded = self.dropout(self.embedding(inputs))\n",
    "            output, (hidden, cell) = self.decoder(embedded, (hidden, cell))\n",
    "            prediction = self.fc_out(output.squeeze(1))\n",
    "            top1 = prediction.argmax(1)\n",
    "            outputs.append(top1.unsqueeze(1))\n",
    "            inputs = top1.unsqueeze(1)\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "VOCAB_SIZE = sp.get_piece_size()\n",
    "EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.7\n",
    "\n",
    "model = Seq2Seq(VOCAB_SIZE, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, PAD_IDX).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_wer(model, dataloader, sp, max_batches=None):\n",
    "    model.eval()\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    batches_processed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            batch_size = src.size(0)\n",
    "            outputs = model.predict(src, max_len=MAX_LEN)\n",
    "            outputs = outputs.cpu().tolist()\n",
    "            trg = trg.cpu().tolist()\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_ids = outputs[i]\n",
    "                trg_ids = trg[i][1:]  # Remove <s>\n",
    "\n",
    "                # Remove PAD and special tokens\n",
    "                pred_ids = [idx for idx in pred_ids if idx not in [PAD_IDX, EOS_IDX, UNK_IDX]]\n",
    "                trg_ids = [idx for idx in trg_ids if idx not in [PAD_IDX, EOS_IDX, UNK_IDX]]\n",
    "\n",
    "                pred_sentence = sp.DecodeIds(pred_ids)\n",
    "                trg_sentence = sp.DecodeIds(trg_ids)\n",
    "\n",
    "                cer_score = cer(trg_sentence, pred_sentence)\n",
    "                wer_score = wer(trg_sentence, pred_sentence)\n",
    "\n",
    "                cer_scores.append(cer_score)\n",
    "                wer_scores.append(wer_score)\n",
    "\n",
    "            batches_processed += 1\n",
    "            if max_batches and batches_processed >= max_batches:\n",
    "                break\n",
    "\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    return avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "train_wers = []\n",
    "valid_wers = []\n",
    "train_cers = []\n",
    "valid_cers = []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for src, trg in tqdm(train_loader, desc=f'Training Epoch {epoch}/{N_EPOCHS}'):\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # output: [batch_size, trg_len - 1, vocab_size]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        # Reshape for loss computation\n",
    "        output = output.reshape(-1, VOCAB_SIZE)\n",
    "        trg = trg[:, 1:].reshape(-1)  # Remove first token (<s>) for target\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (commented out; uncomment to enable)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(val_loader, desc=f'Validation Epoch {epoch}/{N_EPOCHS}'):\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "            output = output.reshape(-1, VOCAB_SIZE)\n",
    "            trg = trg[:, 1:].reshape(-1)  # Remove <BOS>\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_valid_loss += loss.item()\n",
    "\n",
    "    epoch_valid_loss /= len(val_loader)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    \n",
    "    valid_cer, valid_wer = evaluate_wer(model, val_loader, sp)\n",
    "    valid_wers.append(valid_wer)\n",
    "    valid_cers.append(valid_cer)\n",
    "    \n",
    "    train_subset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_cer, train_wer = evaluate_wer(model, train_subset_loader, sp, max_batches=5)  # Evaluate on 5 batches\n",
    "    train_wers.append(train_wer)\n",
    "    \n",
    "    print(f'\\tTrain WER: {train_wer:.4f}')\n",
    "    print(f'\\tValid WER: {valid_wer:.4f}')\n",
    "\n",
    "    print(f'\\tTrain CER: {train_cer:.4f}')\n",
    "    print(f'\\tValid CER: {valid_cer:.4f}')\n",
    "    \n",
    "    # Check if this is the best validation loss so far\n",
    "    if epoch_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f'Validation loss improved. Model saved to {BEST_MODEL_PATH}.')\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(int(end_time - start_time), 60)\n",
    "\n",
    "    print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {epoch_train_loss:.3f}')\n",
    "    print(f'\\tValid Loss: {epoch_valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, N_EPOCHS + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, N_EPOCHS + 1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(LOSS_PLOT_PATH)\n",
    "plt.show()\n",
    "print(f'Loss plot saved to {LOSS_PLOT_PATH}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_wers) + 1), train_wers, label='Train WER')\n",
    "plt.plot(range(1, len(valid_wers) + 1), valid_wers, label='Validation WER')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('WER')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation WER Over Epochs')\n",
    "wer_plot_path = os.path.join(OUTPUT_DIR, 'wer_plot.png')\n",
    "plt.savefig(wer_plot_path)\n",
    "plt.show()\n",
    "print(f'WER plot saved to {wer_plot_path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_cers) + 1), train_cers, label='Train CER')\n",
    "plt.plot(range(1, len(valid_cers) + 1), valid_cers, label='Validation CER')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CER')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation CER Over Epochs')\n",
    "cer_plot_path = os.path.join(OUTPUT_DIR, 'cer_plot.png')\n",
    "plt.savefig(cer_plot_path)\n",
    "plt.show()\n",
    "print(f'CER plot saved to {cer_plot_path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inference\n",
    "def translate_sentence(sentence, model, sp, device, max_len=MAX_LEN):\n",
    "    model.eval()\n",
    "    tokens = [BOS_IDX] + sp.EncodeAsIds(sentence) + [EOS_IDX]\n",
    "    tokens = tokens[:max_len] + [PAD_IDX] * max(0, max_len - len(tokens))\n",
    "    src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.predict(src_tensor, max_len)\n",
    "    outputs = outputs.squeeze(0).tolist()\n",
    "    # Remove PAD and EOS tokens\n",
    "    outputs = [idx for idx in outputs if idx not in [PAD_IDX, EOS_IDX]]\n",
    "    translation = sp.DecodeIds(outputs)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate CER and WER\n",
    "def calculate_metrics(references, hypotheses):\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        cer_score = cer(ref, hyp)\n",
    "        wer_score = wer(ref, hyp)\n",
    "        cer_scores.append(cer_score)\n",
    "        wer_scores.append(wer_score)\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    return avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate and save results\n",
    "def evaluate_and_save(model, df, src_sequences, trg_sequences, sp, file_name):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "\n",
    "    for src_ids, trg_ids in tqdm(zip(src_sequences, trg_sequences), total=len(src_sequences), desc=f'Evaluating {file_name}'):\n",
    "        src_sentence = sp.DecodeIds([id for id in src_ids if id not in [BOS_IDX, EOS_IDX, PAD_IDX]])\n",
    "        trg_sentence = sp.DecodeIds([id for id in trg_ids if id not in [BOS_IDX, EOS_IDX, PAD_IDX]])\n",
    "\n",
    "        pred_sentence = translate_sentence(src_sentence, model, sp, DEVICE)\n",
    "        predictions.append(pred_sentence)\n",
    "        cer_score = cer(trg_sentence, pred_sentence)\n",
    "        wer_score = wer(trg_sentence, pred_sentence)\n",
    "        cer_scores.append(cer_score)\n",
    "        wer_scores.append(wer_score)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Source': df['inFormalForm'],\n",
    "        'Target': df['FormalForm'],\n",
    "        'Prediction': predictions,\n",
    "        'CER': cer_scores,\n",
    "        'WER': wer_scores\n",
    "    })\n",
    "\n",
    "    results_df = results_df.sort_values(by=['CER', 'WER'], ascending=[True, True])\n",
    "\n",
    "    results_path = os.path.join(OUTPUT_DIR, file_name)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    print(f'Results saved to {results_path}')\n",
    "    print(f'Average CER: {avg_cer:.4f}')\n",
    "    print(f'Average WER: {avg_wer:.4f}')\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "print('Best model loaded.')\n",
    "\n",
    "# Evaluate on training data\n",
    "print('Evaluating on training data...')\n",
    "train_results = evaluate_and_save(model, train_df, train_src, train_trg, sp, 'train_results.csv')\n",
    "\n",
    "# Evaluate on validation data\n",
    "print('Evaluating on validation data...')\n",
    "val_results = evaluate_and_save(model, val_df, val_src, val_trg, sp, 'val_results.csv')\n",
    "\n",
    "# Evaluate on test data\n",
    "print('Evaluating on test data...')\n",
    "test_results = evaluate_and_save(model, test_df, test_src, test_trg, sp, 'test_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
