{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from jiwer import wer, cer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = './output_1/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for saving/loading\n",
    "DATASET_PATH = './exportStatements.xlsx'\n",
    "VOCAB_PATH = os.path.join(OUTPUT_DIR, 'word_vocab.pkl')\n",
    "PREPROCESSED_DATA_PATH = os.path.join(OUTPUT_DIR, 'preprocessed_data_word.pkl')\n",
    "BEST_MODEL_PATH = os.path.join(OUTPUT_DIR, 'best_lstm_model_word.pt')\n",
    "BEST_CER_MODEL_PATH = os.path.join(OUTPUT_DIR, 'best_lstm_model_cer.pt')\n",
    "LOSS_PLOT_PATH = os.path.join(OUTPUT_DIR, 'lstm_loss_plot_char.png')\n",
    "WER_PLOT_PATH = os.path.join(OUTPUT_DIR, 'wer_plot_char.png')\n",
    "CER_PLOT_PATH = os.path.join(OUTPUT_DIR, 'cer_plot_char.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_excel(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'inFormalForm' and 'FormalForm'\n",
    "print(\"Missing values in 'inFormalForm':\", df['inFormalForm'].isnull().sum())\n",
    "print(\"Missing values in 'FormalForm':\", df['FormalForm'].isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in 'inFormalForm' and 'FormalForm'\n",
    "initial_length = len(df)\n",
    "df = df.dropna(subset=['inFormalForm', 'FormalForm']).reset_index(drop=True)\n",
    "final_length = len(df)\n",
    "\n",
    "df['inFormalForm'] = df['inFormalForm'].astype(str)\n",
    "df['FormalForm'] = df['FormalForm'].astype(str)\n",
    "\n",
    "print(f\"Dropped {initial_length - final_length} rows due to missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets (80%, 10%, 10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word vocabulary from training data\n",
    "if not os.path.exists(VOCAB_PATH):\n",
    "    print('Building word vocabulary...')\n",
    "    from collections import Counter\n",
    "\n",
    "    # Simple tokenizer function\n",
    "    def tokenize(text):\n",
    "        # Split on whitespace and punctuation\n",
    "        tokens = re.findall(r'\\w+|[^\\s\\w]+', text)\n",
    "        return tokens\n",
    "\n",
    "    # Collect all words from the training data\n",
    "    all_words = []\n",
    "    for text in train_df['inFormalForm'].tolist() + train_df['FormalForm'].tolist():\n",
    "        tokens = tokenize(text)\n",
    "        all_words.extend(tokens)\n",
    "\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(all_words)\n",
    "    words = sorted(word_counts.keys())\n",
    "\n",
    "    # Add special tokens\n",
    "    special_tokens = ['<pad>', '<unk>', '<s>', '</s>']\n",
    "    word2idx = {word: idx + len(special_tokens) for idx, word in enumerate(words)}\n",
    "    for idx, token in enumerate(special_tokens):\n",
    "        word2idx[token] = idx\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    # Save vocabulary\n",
    "    with open(VOCAB_PATH, 'wb') as f:\n",
    "        pickle.dump({'word2idx': word2idx, 'idx2word': idx2word}, f)\n",
    "    print('Word vocabulary built and saved.')\n",
    "else:\n",
    "    print('Loading existing word vocabulary...')\n",
    "    with open(VOCAB_PATH, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        word2idx = vocab['word2idx']\n",
    "        idx2word = vocab['idx2word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token IDs\n",
    "PAD_IDX = word2idx['<pad>']\n",
    "UNK_IDX = word2idx['<unk>']\n",
    "BOS_IDX = word2idx['<s>']\n",
    "EOS_IDX = word2idx['</s>']\n",
    "\n",
    "PAD_IDX, UNK_IDX, BOS_IDX, EOS_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length (based on dataset)\n",
    "def get_max_len(df_list):\n",
    "    max_len = 0\n",
    "    for df in df_list:\n",
    "        lengths_src = df['inFormalForm'].apply(lambda x: len(tokenize(x)) + 2)  # +2 for BOS and EOS\n",
    "        lengths_trg = df['FormalForm'].apply(lambda x: len(tokenize(x)) + 2)\n",
    "        max_len = max(max_len, lengths_src.max(), lengths_trg.max())\n",
    "    return max_len\n",
    "\n",
    "MAX_LEN = get_max_len([train_df, val_df, test_df])\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if preprocessed data exists\n",
    "if not os.path.exists(PREPROCESSED_DATA_PATH):\n",
    "    print('Preprocessing data...')\n",
    "    # Preprocess and tokenize all sentences\n",
    "    def preprocess_data(df, word2idx, max_len=MAX_LEN):\n",
    "        src_texts = df['inFormalForm'].tolist()\n",
    "        trg_texts = df['FormalForm'].tolist()\n",
    "        src_sequences = []\n",
    "        trg_sequences = []\n",
    "        for src, trg in zip(src_texts, trg_texts):\n",
    "            src_tokens = tokenize(src)\n",
    "            trg_tokens = tokenize(trg)\n",
    "            src_ids = [BOS_IDX] + [word2idx.get(w, UNK_IDX) for w in src_tokens] + [EOS_IDX]\n",
    "            trg_ids = [BOS_IDX] + [word2idx.get(w, UNK_IDX) for w in trg_tokens] + [EOS_IDX]\n",
    "            # Pad or truncate sequences\n",
    "            src_ids = src_ids[:max_len] + [PAD_IDX] * max(0, max_len - len(src_ids))\n",
    "            trg_ids = trg_ids[:max_len] + [PAD_IDX] * max(0, max_len - len(trg_ids))\n",
    "            src_sequences.append(src_ids)\n",
    "            trg_sequences.append(trg_ids)\n",
    "        return src_sequences, trg_sequences\n",
    "    \n",
    "    # Tokenize and preprocess data\n",
    "    train_src, train_trg = preprocess_data(train_df, word2idx)\n",
    "    val_src, val_trg = preprocess_data(val_df, word2idx)\n",
    "    test_src, test_trg = preprocess_data(test_df, word2idx)\n",
    "\n",
    "    # Save preprocessed data\n",
    "    with open(PREPROCESSED_DATA_PATH, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'train_src': train_src,\n",
    "            'train_trg': train_trg,\n",
    "            'val_src': val_src,\n",
    "            'val_trg': val_trg,\n",
    "            'test_src': test_src,\n",
    "            'test_trg': test_trg,\n",
    "            'MAX_LEN': MAX_LEN\n",
    "        }, f)\n",
    "    print('Preprocessed data saved.')\n",
    "else:\n",
    "    print('Loading preprocessed data...')\n",
    "    # Load preprocessed data\n",
    "    with open(PREPROCESSED_DATA_PATH, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        train_src = data['train_src']\n",
    "        train_trg = data['train_trg']\n",
    "        val_src = data['val_src']\n",
    "        val_trg = data['val_trg']\n",
    "        test_src = data['test_src']\n",
    "        test_trg = data['test_trg']\n",
    "        MAX_LEN = data['MAX_LEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sequences, trg_sequences):\n",
    "        self.src_sequences = src_sequences\n",
    "        self.trg_sequences = trg_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = torch.tensor(self.src_sequences[idx], dtype=torch.long)\n",
    "        trg_ids = torch.tensor(self.trg_sequences[idx], dtype=torch.long)\n",
    "        return src_ids, trg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_trg)\n",
    "val_dataset = TranslationDataset(val_src, val_trg)\n",
    "test_dataset = TranslationDataset(test_src, test_trg)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM-based Seq2Seq model (same as before)\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.encoder = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # Embed source and target sequences\n",
    "        embedded_src = self.dropout(self.embedding(src))\n",
    "        embedded_trg = self.dropout(self.embedding(trg[:, :-1]))  # Remove last token for decoder input\n",
    "\n",
    "        # Encode source sequence\n",
    "        _, (hidden, cell) = self.encoder(embedded_src)\n",
    "\n",
    "        # Decode target sequence\n",
    "        outputs, _ = self.decoder(embedded_trg, (hidden, cell))\n",
    "        predictions = self.fc_out(outputs)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, src, max_len=MAX_LEN):\n",
    "        # Embed source sequence\n",
    "        embedded_src = self.dropout(self.embedding(src))\n",
    "\n",
    "        # Encode source sequence\n",
    "        _, (hidden, cell) = self.encoder(embedded_src)\n",
    "\n",
    "        batch_size = src.size(0)\n",
    "        # Initialize target sequence with <s>\n",
    "        inputs = torch.tensor([BOS_IDX] * batch_size, dtype=torch.long).unsqueeze(1).to(src.device)\n",
    "        outputs = []\n",
    "        # Keep track of finished sequences\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool).to(src.device)\n",
    "        for _ in range(max_len):\n",
    "            embedded = self.dropout(self.embedding(inputs))\n",
    "            output, (hidden, cell) = self.decoder(embedded, (hidden, cell))\n",
    "            prediction = self.fc_out(output.squeeze(1))\n",
    "            top1 = prediction.argmax(dim=1)  # Shape: [batch_size]\n",
    "            outputs.append(top1.unsqueeze(1))\n",
    "            inputs = top1.unsqueeze(1)\n",
    "\n",
    "            # Update finished sequences\n",
    "            eos_found = top1 == EOS_IDX\n",
    "            finished = finished | eos_found  # Logical OR to update finished sequences\n",
    "            if finished.all():\n",
    "                break\n",
    "        outputs = torch.cat(outputs, dim=1)  # Shape: [batch_size, seq_len]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "EMB_DIM = 256  # Same as before\n",
    "HID_DIM = 512  # Same as before\n",
    "N_LAYERS = 2   # Same as before\n",
    "DROPOUT = 0.5  # Same as before\n",
    "\n",
    "model = Seq2Seq(VOCAB_SIZE, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, PAD_IDX).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_wer(model, dataloader, idx2word, max_batches=None):\n",
    "    model.eval()\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    batches_processed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            batch_size = src.size(0)\n",
    "            outputs = model.predict(src, max_len=MAX_LEN)\n",
    "            outputs = outputs.cpu().tolist()\n",
    "            trg = trg.cpu().tolist()\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                pred_ids = outputs[i]\n",
    "                trg_ids = trg[i][1:]  # Remove <s>\n",
    "\n",
    "               # Remove PAD and special tokens\n",
    "                pred_ids = [idx for idx in pred_ids if idx not in [PAD_IDX, EOS_IDX, UNK_IDX]]\n",
    "                trg_ids = [idx for idx in trg_ids if idx not in [PAD_IDX, EOS_IDX, UNK_IDX]]\n",
    "\n",
    "                pred_sentence = ' '.join([idx2word.get(idx, '') for idx in pred_ids])\n",
    "                trg_sentence = ' '.join([idx2word.get(idx, '') for idx in trg_ids])\n",
    "\n",
    "                cer_score = cer(trg_sentence, pred_sentence)\n",
    "                wer_score = wer(trg_sentence, pred_sentence)\n",
    "\n",
    "                cer_scores.append(cer_score)\n",
    "                wer_scores.append(wer_score)\n",
    "\n",
    "            batches_processed += 1\n",
    "            if max_batches and batches_processed >= max_batches:\n",
    "                break\n",
    "\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    return avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with WER calculation (same as before)\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1  # Enable gradient clipping\n",
    "best_valid_loss = float('inf')\n",
    "best_valid_cer = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "train_wers = []\n",
    "valid_wers = []\n",
    "train_cers = []\n",
    "valid_cers = []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for src, trg in tqdm(train_loader, desc=f'Training Epoch {epoch}/{N_EPOCHS}'):\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # output: [batch_size, trg_len - 1, vocab_size]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        # Reshape for loss computation\n",
    "        output = output.reshape(-1, VOCAB_SIZE)\n",
    "        trg = trg[:, 1:].reshape(-1)  # Remove first token (<s>) for target\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(val_loader, desc=f'Validation Epoch {epoch}/{N_EPOCHS}'):\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "            output = output.reshape(-1, VOCAB_SIZE)\n",
    "            trg = trg[:, 1:].reshape(-1)  # Remove first token (<s>) for target\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_valid_loss += loss.item()\n",
    "\n",
    "    epoch_valid_loss /= len(val_loader)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    \n",
    "    valid_cer, valid_wer = evaluate_wer(model, val_loader, idx2word)\n",
    "    valid_wers.append(valid_wer)\n",
    "    valid_cers.append(valid_cer)\n",
    "\n",
    "    train_subset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_cer, train_wer = evaluate_wer(model, train_subset_loader, idx2word, max_batches=5)\n",
    "    train_wers.append(train_wer)\n",
    "    train_cers.append(train_cer)\n",
    "    \n",
    "    print(f'\\tTrain Loss: {epoch_train_loss:.3f}')\n",
    "    print(f'\\tValid Loss: {epoch_valid_loss:.3f}')\n",
    "    \n",
    "    print(f'\\tTrain WER: {train_wer:.4f}')\n",
    "    print(f'\\tValid WER: {valid_wer:.4f}')\n",
    "\n",
    "    print(f'\\tTrain CER: {train_cer:.4f}')\n",
    "    print(f'\\tValid CER: {valid_cer:.4f}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if epoch_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f'Validation loss improved. Model saved to {BEST_MODEL_PATH}.')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "    if valid_cer < best_valid_cer:\n",
    "        best_valid_cer = valid_cer\n",
    "        torch.save(model.state_dict(), BEST_CER_MODEL_PATH)\n",
    "        print(f'Validation CER improved. Model saved to {BEST_CER_MODEL_PATH}.')\n",
    "            \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(int(end_time - start_time), 60)\n",
    "\n",
    "    print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss (same as before)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(valid_losses) + 1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(LOSS_PLOT_PATH)\n",
    "plt.show()\n",
    "print(f'Loss plot saved to {LOSS_PLOT_PATH}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_wers) + 1), train_wers, label='Train WER')\n",
    "plt.plot(range(1, len(valid_wers) + 1), valid_wers, label='Validation WER')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('WER')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation WER Over Epochs')\n",
    "plt.savefig(WER_PLOT_PATH)\n",
    "plt.show()\n",
    "print(f'WER plot saved to {WER_PLOT_PATH}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_cers) + 1), train_cers, label='Train CER')\n",
    "plt.plot(range(1, len(valid_cers) + 1), valid_cers, label='Validation CER')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CER')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation CER Over Epochs')\n",
    "plt.savefig(CER_PLOT_PATH)\n",
    "plt.show()\n",
    "print(f'CER plot saved to {CER_PLOT_PATH}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inference\n",
    "def translate_sentence(sentence, model, word2idx, idx2word, device, max_len=MAX_LEN):\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)\n",
    "    tokens = [BOS_IDX] + [word2idx.get(w, UNK_IDX) for w in tokens] + [EOS_IDX]\n",
    "    tokens = tokens[:max_len]\n",
    "    src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.predict(src_tensor, max_len)\n",
    "    outputs = outputs.squeeze(0).tolist()\n",
    "    # Remove BOS token\n",
    "    outputs = outputs\n",
    "    # Stop at EOS token\n",
    "    if EOS_IDX in outputs:\n",
    "        eos_index = outputs.index(EOS_IDX)\n",
    "        outputs = outputs[:eos_index]\n",
    "    translation = ' '.join([idx2word.get(idx, '') for idx in outputs if idx not in [PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX]])\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate CER and WER\n",
    "def calculate_metrics(references, hypotheses):\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        cer_score = cer(ref, hyp)\n",
    "        wer_score = wer(ref, hyp)\n",
    "        cer_scores.append(cer_score)\n",
    "        wer_scores.append(wer_score)\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    return avg_cer, avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and save results\n",
    "def evaluate_and_save(model, df, src_sequences, trg_sequences, word2idx, idx2word, file_name):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "\n",
    "    for src_ids, trg_ids in tqdm(zip(src_sequences, trg_sequences), total=len(src_sequences), desc=f'Evaluating {file_name}'):\n",
    "        src_sentence = ' '.join([idx2word.get(idx, '') for idx in src_ids if idx not in [BOS_IDX, EOS_IDX, PAD_IDX]])\n",
    "        trg_sentence = ' '.join([idx2word.get(idx, '') for idx in trg_ids if idx not in [BOS_IDX, EOS_IDX, PAD_IDX]])\n",
    "\n",
    "        pred_sentence = translate_sentence(src_sentence, model, word2idx, idx2word, DEVICE)\n",
    "        predictions.append(pred_sentence)\n",
    "        cer_score = cer(trg_sentence, pred_sentence)\n",
    "        wer_score = wer(trg_sentence, pred_sentence)\n",
    "        cer_scores.append(cer_score)\n",
    "        wer_scores.append(wer_score)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Source': df['inFormalForm'],\n",
    "        'Target': df['FormalForm'],\n",
    "        'Prediction': predictions,\n",
    "        'CER': cer_scores,\n",
    "        'WER': wer_scores\n",
    "    })\n",
    "    \n",
    "    results_df = results_df.sort_values(by=['CER', 'WER'], ascending=[True, True])\n",
    "\n",
    "    results_path = os.path.join(OUTPUT_DIR, file_name)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    avg_cer = np.mean(cer_scores)\n",
    "    avg_wer = np.mean(wer_scores)\n",
    "    print(f'Results saved to {results_path}')\n",
    "    print(f'Average CER: {avg_cer:.4f}')\n",
    "    print(f'Average WER: {avg_wer:.4f}')\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "print('Best model loaded.')\n",
    "\n",
    "# Evaluate on training data\n",
    "print('Evaluating on training data...')\n",
    "train_results = evaluate_and_save(model, train_df, train_src, train_trg, word2idx, idx2word, 'train_results_word.csv')\n",
    "\n",
    "# Evaluate on validation data\n",
    "print('Evaluating on validation data...')\n",
    "val_results = evaluate_and_save(model, val_df, val_src, val_trg, word2idx, idx2word, 'val_results_word.csv')\n",
    "\n",
    "# Evaluate on test data\n",
    "print('Evaluating on test data...')\n",
    "test_results = evaluate_and_save(model, test_df, test_src, test_trg, word2idx, idx2word, 'test_results_word.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best CER model\n",
    "model.load_state_dict(torch.load(BEST_CER_MODEL_PATH))\n",
    "print('Best CER model loaded.')\n",
    "\n",
    "# Evaluate on training data\n",
    "print('Evaluating on training data using best CER model...')\n",
    "train_results = evaluate_and_save(model, train_df, train_src, train_trg, word2idx, idx2word, 'train_results_best_cer.csv')\n",
    "\n",
    "# Evaluate on validation data\n",
    "print('Evaluating on validation data using best CER model...')\n",
    "val_results = evaluate_and_save(model, val_df, val_src, val_trg, word2idx, idx2word, 'val_results_best_cer.csv')\n",
    "\n",
    "# Evaluate on test data using the best CER model\n",
    "print('Evaluating on test data using best CER model...')\n",
    "test_results = evaluate_and_save(model, test_df, test_src, test_trg, word2idx, idx2word, 'test_results_best_cer.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
